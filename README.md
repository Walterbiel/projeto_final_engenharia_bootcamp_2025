# üöÄ Projeto Final ‚Äì Data Warehouse com dbt, Docker e Airflow

Este projeto tem como objetivo construir um **Data Warehouse completo**, utilizando **PostgreSQL**, **dbt**, **Docker** e **Airflow**, seguindo boas pr√°ticas de engenharia de dados.

---

## 1Ô∏è‚É£ Setup do Ambiente Local

### Pr√©-requisitos

- Python instalado  
- Acesso ao terminal (PowerShell ou Git Bash)  
- Docker e Docker Desktop instalados  

---

### Estrutura de Pastas do Projeto

Criar as seguintes pastas na raiz do reposit√≥rio:

```
1_local_setup
2_data_warehouse
3_airflow
```

---

### Setup do Ambiente Python

Instalar o `uv` (gerenciador de depend√™ncias e ambientes virtuais):

```bash
pip install uv
uv --version
```

Entrar na pasta de setup local:

```bash
cd .\1_local_setup\
```

Criar e ativar o ambiente virtual:

```bash
uv venv .venv
.\.venv\Scripts\Activate.ps1
```
---
```bash
uv init
```

> ‚ö†Ô∏è **Importante**  
> Sempre execute os comandos **nesta ordem** para evitar problemas de ambiente.

---

### Instala√ß√£o das Depend√™ncias

```bash
uv add dbt-core dbt-postgres faker pandas numpy
```

---

## üê≥ Docker

### Docker Compose

- Criar o arquivo `docker-compose.yml` dentro da pasta `1_local_setup`
- Respons√°vel por subir o PostgreSQL local

---

### Arquivo `.env`

Criar o arquivo `.env` dentro da pasta `1_local_setup`:

```env
DBT_USER=postgres
DBT_PASSWORD=postgres
```

Adicionar o arquivo `.env` ao `.gitignore`.

---

### Subir o Ambiente Docker

```bash
docker compose up -d
```

Validar se o container est√° rodando no Docker Desktop.

---

## 2Ô∏è‚É£ Data Warehouse com dbt

Entrar na pasta:

```bash
cd 2_data_warehouse

..\1_local_setup\.venv\Scripts\activate
```

Inicializar o projeto dbt:

```bash
dbt init
```

---

### Configura√ß√£o Interativa do dbt

Durante a inicializa√ß√£o:

- Nome do projeto  
- Op√ß√£o: PostgreSQL  
- Host: `localhost`  
- Porta: `5433`  
- User: conforme `.env`  
- Senha: conforme `.env`  
- Database: `dbt_db`  
- Schema: `public`  
- Threads: `4`  

---

### Valida√ß√£o da Conex√£o

```bash
cd dw_bootcamp
dbt debug
```

---

## Seeds

- Colocar arquivos CSV dentro da pasta `seeds/`
- Seeds representam dados de origem para estudo e testes

---

## Configura√ß√£o do `dbt_project.yml`

```yml
name: 'dw_bootcamp'
version: '1.0.0'

profile: 'dw_bootcamp'

model-paths: ["models"]
analysis-paths: ["analyses"]
test-paths: ["tests"]
seed-paths: ["seeds"]
macro-paths: ["macros"]
snapshot-paths: ["snapshots"]

vars:
  "dbt_date:time_zone": "America/Sao_Paulo"

clean-targets:
  - "target"
  - "dbt_packages"

models:
  dw_bootcamp:
    staging:
      +materialized: view
    intermediate:
      +materialized: table
    mart:
      +materialized: table
```

---

## Estrutura de Models

```
models/
‚îú‚îÄ‚îÄ staging
‚îú‚îÄ‚îÄ intermediate
‚îî‚îÄ‚îÄ mart
```

- **staging**: limpeza e padroniza√ß√£o dos dados
- **intermediate**: cria√ß√£o de fatos e dimens√µes
- **mart**: modelos finais prontos para an√°lise

---

## Execu√ß√£o do dbt

Pipeline completo:

```bash
dbt build
```

Rodar apenas um modelo espec√≠fico:

```bash
dbt run -s stg_airline_delay_cause
```

Para rodar sem executar seeds novamente:

```bash
dbt build --exclude-resource-type seed
```

---

## Pacotes do dbt

Criar o arquivo `packages.yml`:

```yml
packages:

  - package: dbt-labs/dbt_utils
    version: "1.3.0"

  - package: metaplane/dbt_expectations
    version: "0.10.8"
```

Instalar os pacotes:

```bash
dbt deps
```

---

## 3Ô∏è‚É£ Airflow

Instalar o Astro CLI:

```bash
winget install -e --id Astronomer.Astro
```

Requeriments.txt do airflow:

'''
apache-airflow-providers-postgres
astronomer-cosmos
'''


Criar dag python:

```python
# Importa o Variable do Airflow, que permite ler vari√°veis configuradas na UI do Airflow
# (ex: escolher se o DAG roda em dev ou prod sem mudar o c√≥digo)
from airflow.models import Variable

# Importa os componentes do Cosmos, biblioteca que transforma um projeto dbt em um DAG no Airflow
# - DbtDag: cria o DAG automaticamente com base nos modelos do dbt
# - ProjectConfig: aponta onde est√° o projeto dbt
# - ProfileConfig: define qual profile/target do dbt ser√° usado
# - ExecutionConfig: define como executar o dbt (caminho do execut√°vel)
from cosmos import DbtDag, ProjectConfig, ProfileConfig, ExecutionConfig

# Importa o mapeamento de profile para Postgres usando usu√°rio/senha a partir de uma conex√£o do Airflow
# (Cosmos ‚Äútraduz‚Äù uma Airflow Connection em um profiles.yml em tempo de execu√ß√£o)
from cosmos.profiles import PostgresUserPasswordProfileMapping

# Importa os para lidar com vari√°veis de ambiente e caminhos
import os

# Importa datetime do pendulum (o Airflow usa pendulum para datas/timezones de forma mais robusta)
from pendulum import datetime


# =========================
# 1) CONFIG DE PROFILE DEV
# =========================
# Cria um ProfileConfig para o ambiente dev
# - profile_name: nome do profile do dbt (equivalente ao que existiria no profiles.yml)
# - target_name: target do dbt (dev)
# - profile_mapping: como o Cosmos vai montar as credenciais do dbt usando uma conex√£o do Airflow
profile_config_dev = ProfileConfig(
    profile_name="dw_bootcamp",
    target_name="dev",
    profile_mapping=PostgresUserPasswordProfileMapping(
        # conn_id: nome da conex√£o cadastrada no Airflow (Admin -> Connections)
        # Essa conex√£o deve apontar para o Postgres do Docker local
        conn_id="docker_postgres_db",
        # profile_args: argumentos extras do profile do dbt
        # Aqui estamos for√ßando o schema a ser "public"
        profile_args={"schema": "public"},
    ),
)


# ==========================
# 2) CONFIG DE PROFILE PROD
# ==========================
# Cria um ProfileConfig para o ambiente prod
# Aqui a diferen√ßa principal √© a conex√£o do Airflow (conn_id)
# que deve apontar para o Postgres em ambiente remoto (ex: Railway)
profile_config_prod = ProfileConfig(
    profile_name="dw_bootcamp",
    target_name="prod",
    profile_mapping=PostgresUserPasswordProfileMapping(
        # Conex√£o do Airflow para o Postgres remoto (produ√ß√£o)
        conn_id="railway_postgres_db",
        # Mesmo schema para o dbt
        profile_args={"schema": "public"},
    ),
)


# ======================================
# 3) DEFINIR QUAL AMBIENTE VAI EXECUTAR
# ======================================
# L√™ a vari√°vel "dbt_env" do Airflow
# - Se n√£o existir, usa "dev" como padr√£o
# - lower() garante que n√£o importa se o usu√°rio digitou DEV/Dev/dev
dbt_env = Variable.get("dbt_env", default_var="dev").lower()

# Valida a vari√°vel para evitar valores errados (ex: "teste", "local", etc.)
# Se n√£o for dev ou prod, dispara erro e o DAG n√£o sobe corretamente
if dbt_env not in ("dev", "prod"):
    raise ValueError(f"dbt_env inv√°lido: {dbt_env!r}, use 'dev' ou 'prod'")

# Escolhe qual profile_config ser√° usado com base no ambiente
# - dev -> profile_config_dev
# - prod -> profile_config_prod
profile_config = profile_config_dev if dbt_env == "dev" else profile_config_prod


# ======================================
# 4) CRIAR O DAG DO DBT COM O COSMOS
# ======================================
# DbtDag cria automaticamente as tasks do dbt (run/test/etc) com base no projeto dbt
my_cosmos_dag = DbtDag(

    # -----------------------------
    # 4.1) Configura√ß√£o do projeto
    # -----------------------------
    project_config=ProjectConfig(
        # Caminho onde o projeto dbt est√° dentro do container do Airflow
        # (ex: voc√™ copiou para /usr/local/airflow/dbt/dw_bootcamp)
        dbt_project_path="/usr/local/airflow/dbt/dw_bootcamp",
        # Nome do projeto dbt (mesmo do dbt_project.yml)
        project_name="dw_bootcamp",
    ),

    # -----------------------------
    # 4.2) Configura√ß√£o do profile
    # -----------------------------
    # Aqui o Cosmos injeta as credenciais baseado na conex√£o do Airflow escolhida
    profile_config=profile_config,

    # --------------------------------
    # 4.3) Como executar o dbt no DAG
    # --------------------------------
    execution_config=ExecutionConfig(
        # Caminho do execut√°vel dbt dentro do container
        # Aqui voc√™ est√° usando um virtualenv (dbt_venv) criado no Dockerfile
        dbt_executable_path=f"{os.environ['AIRFLOW_HOME']}/dbt_venv/bin/dbt",
    ),

    # -----------------------------------------
    # 4.4) Argumentos adicionais do operador
    # -----------------------------------------
    operator_args={
        # install_deps=True faz o Cosmos rodar "dbt deps" antes da execu√ß√£o
        # Isso garante que pacotes do packages.yml (dbt_utils, dbt_expectations) sejam baixados
        "install_deps": True,

        # target: define qual target do dbt ser√° usado (dev ou prod)
        # Est√° ligado ao profile_config selecionado
        "target": profile_config.target_name,
    },

    # -----------------------------
    # 4.5) Agendamento do DAG
    # -----------------------------
    # "@daily" significa que o DAG roda uma vez por dia
    schedule="@daily",

    # Data a partir da qual o Airflow considera o DAG v√°lido para agendar execu√ß√µes
    start_date=datetime(2025, 12, 15),

    # catchup=False impede que o Airflow tente executar dias passados automaticamente
    # (sen√£o ele criaria execu√ß√µes retroativas desde start_date at√© hoje)
    catchup=False,

    # ID do DAG no Airflow
    # Aqui voc√™ est√° colocando o ambiente no nome para ficar claro no painel
    dag_id=f"dag_dw_bootcamp_{dbt_env}",

    # Configura√ß√£o padr√£o de tentativas em caso de falha
    default_args={"retries": 2},

    # Coment√°rio livre seu
    #atualizando
)

```

---
Arrastar pasta do projeto dbt na pasta 2 para dbt na pasta 3 (apenas copiar)
---

Inicializar o Airflow:

```bash
cd 3_airflow
astro dev init


astro dev start
```

reiniciar caso necessario:
'''
cd 3_airflow
astro dev stop
astro dev start --no-cache
'''


---

### Dockerfile do Airflow

Adicionar:

```dockerfile
RUN python -m venv dbt_venv \
    && . dbt_venv/bin/activate \
    && pip install --no-cache-dir dbt-postgres==1.9.0 \
    && deactivate
```

---

### Docker Compose Override

```yml
services:
  scheduler:
    volumes:
      - ./dbt/dw_bootcamp:/usr/local/airflow/dbt/dw_bootcamp

  dag-processor:
    volumes:
      - ./dbt/dw_bootcamp:/usr/local/airflow/dbt/dw_bootcamp
```

---

### Documenta√ß√£o do dbt

```bash
cd ..\2_data_warehouse\dw_bootcamp
dbt docs generate --target dev
dbt docs serve --port 8085
```


---
3Ô∏è‚É£ Agora crie a conex√£o 

Airflow UI ‚Üí Admin ‚Üí Connections ‚Üí Add

Agora voc√™ ver√°:

‚úÖ Connection Type: Postgres

Preencha assim:

Connection Id: docker_postgres_db

Connection Type: Postgres

Host: host.docker.internal

Schema (Database): dbt_db

Login: postgres

Password: postgres

Port: 5433